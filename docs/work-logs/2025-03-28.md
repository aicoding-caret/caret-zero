# 일일 작업 일지: 2025-03-28

**어제 완료된 작업:**

-   Ollama 멀티 GPU 지원 제한 가능성 및 대안 조사 (vLLM, TGI 등).
-   초기에는 TGI(Text Generation Inference)를 테스트하기로 결정했었음.

**오늘의 작업 계획:**

1.  **태스크 #004 진행 (vLLM으로 전환):** vLLM 설정 및 성능 테스트 ([#004](/docs/work-logs/luke-and-alpha/tasks/004-tgi-setup-and-test.md))
    -   vLLM 설치 (Docker 권장) 및 GPU 설정 (`tensor-parallel-size 2`).
    -   `Qwen/Qwen2.5-Coder-32B-Instruct` 모델 로딩 및 API 엔드포인트 확인.
    -   sLLM 테스트 스크립트 수정 (vLLM 지원 추가).
    -   vLLM 환경에서 성능 테스트 실행.
    -   결과 분석 및 Ollama 결과와 비교.

**진행 상황:**

-   [x] 태스크 #004: vLLM 설정 및 성능 테스트 (TGI에서 전환) **(완료)**
    -   [x] 관련 문서 (태스크 정의, 설정 가이드) vLLM 기준으로 수정
    -   [x] 테스트 스크립트 (`run_batch_test.ps1`, `run_batch_test.sh`, `run_test.py`) vLLM 기준으로 수정 (vLLM API, AWQ 지원, 의존성 추가 등)
    -   [x] vLLM 설치 및 설정 (Docker 이미지 `vllm/vllm-openai:v0.5.1` 사용)
    -   [x] vLLM을 통해 `Qwen/Qwen2.5-Coder-32B-Instruct-AWQ` 모델 로딩 성공 (FP16 버전은 OOM 발생)
    -   [ ] 성능 테스트 실행
    -   [x] 성능 테스트 실행 (빠른 테스트 진행)
        -   [x] `run_test.py`에 `--limit-scenarios`, `--limit-prompts` 옵션 추가
        -   [x] `run_batch_test.ps1` 수정하여 빠른 테스트 (1회 반복, 3개 모델, 제한 옵션 적용) 실행
        -   [x] `AWQ` 모델 빠른 테스트 성공
        -   [x] `GPTQ-Int4` 모델 빠른 테스트 성공 (초기 시도 시 타임아웃 후 `gptq` 옵션 재확인 후 성공)
        -   [x] `GPTQ-Int8` 모델 빠른 테스트 진행 완료 (로딩 시간 소요 후 완료) **(완료)**
    -   [x] 결과 분석 및 비교 **(완료, [보고서](./luke-and-alpha/reports/004-ollama-vs-vllm-performance-comparison.md) 참고)**

**특이사항:**

-   **TGI OOM:** Qwen 32B AWQ 모델 로딩 시 TGI 환경에서 OOM 발생.
-   **vLLM 전환:** TGI 문제 및 vLLM의 Qwen 지원을 고려하여 vLLM으로 전환.
-   **vLLM FP16 OOM:** vLLM 환경에서도 `Qwen/Qwen2.5-Coder-32B-Instruct` (FP16) 모델은 2x RTX 3090에서 OOM 발생.
-   **vLLM AWQ 성공:** `Qwen/Qwen2.5-Coder-32B-Instruct-AWQ` 모델은 다음 설정으로 vLLM 로딩 성공:
    -   vLLM 이미지: `vllm/vllm-openai:v0.5.1`
    -   주요 옵션: `--tensor-parallel-size 2`, `--quantization awq`, `--dtype float16`, `--shm-size 16g`, `--trust-remote-code`
    -   환경 변수: `VLLM_DISABLE_TORCH_COMPILE=1`, `NCCL_P2P_DISABLE=1` (WSL 환경 고려)
-   **Python 의존성:** `run_test.py` 실행 위해 `requirements.txt`에 `openai`, `psutil`, `GPUtil` 추가 필요했음.

**다음 단계:**

-   vLLM 환경에서 `Qwen/Qwen2.5-Coder-32B-Instruct-AWQ` 모델 성능 테스트 진행.
-   Ollama 결과와 비교 분석.
-   **vLLM GPTQ 로딩:** GPTQ 모델(Int4, Int8) 로딩 시 vLLM에서 시간이 오래 걸리거나 타임아웃 발생하는 경향 확인. 모델 카드에서 `gptq_marlin` 옵션 사용 권장 확인했으나, 실제 적용 시 설정 충돌 오류 발생. 다시 `gptq` 옵션으로 복구 후 Int4는 성공, Int8은 진행 중.
-   **vLLM 메모리:** Ollama와 달리 공유 GPU 메모리 사용량이 거의 없는 특징 관찰.

-   **알파(AI) 동작 규칙 확인:**
    -   ACT MODE에서 알파가 도구 사용 없이 텍스트 응답만 할 경우, 시스템 규칙에 따라 오류가 발생하고 강제로 다음 요청 시 "도구를 사용하지 않았으니 다시 시도하라"는 메시지가 추가됨 (`src/core/Cline.ts`의 `recursivelyMakeClineRequests` 함수 내 `if (!didToolUse)` 블록).
    -   이 규칙 때문에 알파가 스스로 '1분 대기' 같은 시간 지연 동작을 수행하기 어려움.
    -   향후 Cline 프로젝트 코드 수정을 통해 알파가 시간 제어(예: 지정된 시간 동안 대기)를 할 수 있도록 개선하는 것을 고려.

-   **테스트 계획 변경:** Ollama 테스트가 예상보다 오래 걸려 중지함. 완료된 1회차 결과와 vLLM 테스트(AWQ, GPTQ-Int4, GPTQ-Int8) 결과를 비교하기로 결정.
-   **밤샘 테스트 계획:**
    -   테스트 모델: `AWQ`, `GPTQ-Int4` (2종)
    -   반복 횟수: 3회
    -   테스트 범위: 모든 시나리오 및 프롬프트 (`--limit...` 옵션 제거)
    -   자동 종료: 테스트 완료, 보고서 작성 후 컴퓨터 자동 종료 (`shutdown /s /t 0` 명령어 추가 예정)

---
*마지막 업데이트: 2025-03-29 00:27*
